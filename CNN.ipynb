{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 111,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9aFvBs8S0FR7",
        "outputId": "bbf26106-fccb-4f0a-b6f6-68e6fd6c4fd6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import time\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from sklearn.model_selection import ParameterGrid"
      ],
      "metadata": {
        "id": "sN42Q6nA0tKQ"
      },
      "execution_count": 112,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import torchvision.transforms as transforms\n",
        "from torchvision import datasets\n",
        "\n",
        "# Define transformations for data augmentation for training data only\n",
        "transform_train = transforms.Compose([\n",
        "    transforms.RandomRotation(15),             # Randomly rotate the image by up to 15 degrees\n",
        "    transforms.RandomHorizontalFlip(),         # Randomly flip the image horizontally\n",
        "    transforms.RandomCrop(size=28, padding=4), # Randomly crop the image with padding of 4 pixels\n",
        "    transforms.ToTensor(),                     # Convert the image to tensor\n",
        "])\n",
        "\n",
        "# Define transformation for validation and test set (no augmentation)\n",
        "transform_test = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "])\n",
        "\n",
        "# Mount Google Drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Load the .npz file from Google Drive\n",
        "data = np.load('/content/drive/MyDrive/imbalanced_mnist.npz')\n",
        "\n",
        "# Extract the arrays from the loaded data\n",
        "X_train = data['X_train.npy']\n",
        "X_test = data['X_test.npy']\n",
        "y_train = data['y_train.npy']\n",
        "y_test = data['y_test.npy']\n",
        "\n",
        "# Convert numpy arrays to PyTorch tensors\n",
        "X_train_tensor = torch.from_numpy(X_train)\n",
        "y_train_tensor = torch.from_numpy(y_train)\n",
        "X_test_tensor = torch.from_numpy(X_test)\n",
        "y_test_tensor = torch.from_numpy(y_test)\n",
        "\n",
        "# Create datasets\n",
        "train_dataset = torch.utils.data.TensorDataset(X_train_tensor, y_train_tensor)\n",
        "test_dataset = torch.utils.data.TensorDataset(X_test_tensor, y_test_tensor)\n",
        "\n",
        "# Apply transformations only to the training dataset\n",
        "train_dataset.transform = transform_train\n",
        "\n",
        "# Get the indices of samples in the dataset\n",
        "indices = list(range(len(test_dataset)))\n",
        "\n",
        "# Split the indices into validation and testing sets\n",
        "val_indices, test_indices = train_test_split(indices, test_size=0.5, random_state=42)\n",
        "\n",
        "\n",
        "\n",
        "#Generating data loaders from the corresponding datasets\n",
        "batch_size = 128\n",
        "train_loader = torch.utils.data.DataLoader(train_dataset, shuffle=True, batch_size=batch_size)  # MMMR --- Shuffle needs to be true.\n",
        "\n",
        "# Create DataLoader for validation set\n",
        "val_sampler = torch.utils.data.SubsetRandomSampler(val_indices)\n",
        "valid_loader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, sampler=val_sampler)\n",
        "\n",
        "# Create DataLoader for testing set\n",
        "test_sampler = torch.utils.data.SubsetRandomSampler(test_indices)\n",
        "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, sampler=test_sampler)\n",
        "#test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5c4GLSxu7LfN",
        "outputId": "ab693ad8-62fc-468b-e362-5009f3eff1dc"
      },
      "execution_count": 115,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "19oOD7RePL00",
        "outputId": "0f460cfe-cd62-4275-b714-8a6e640fe4b1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor([[  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "            0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
              "         [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "            0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
              "         [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "            0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
              "         [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "            3, 121, 202, 243,  95,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
              "         [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   5,\n",
              "          173, 212,  45,  31,  41,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
              "         [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   7, 145,\n",
              "          212,  22,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
              "         [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0, 115, 221,\n",
              "           23,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
              "         [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  61, 239,  99,\n",
              "            0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
              "         [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  10, 189, 188,   0,\n",
              "            0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
              "         [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  50, 255,  99,   0,\n",
              "            0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
              "         [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0, 119, 202,   3,   0,\n",
              "            0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
              "         [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0, 214, 146,   0,   0,\n",
              "            0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
              "         [  0,   0,   0,   0,   0,   0,   0,   0,   0,  41, 253,  62,   0,   0,\n",
              "            0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
              "         [  0,   0,   0,   0,   0,   0,   0,   0,   0,  71, 238,  21,   0,   0,\n",
              "            0,   0,   4,   9,   2,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
              "         [  0,   0,   0,   0,   0,   0,   0,   0,   0, 124, 205,   0,   0,   0,\n",
              "            0,  44, 210, 249, 160,  25,   0,   0,   0,   0,   0,   0,   0,   0],\n",
              "         [  0,   0,   0,   0,   0,   0,   0,   0,   0, 124, 205,   0,   0,   8,\n",
              "          105, 188,  89,  45, 117, 209,  18,   0,   0,   0,   0,   0,   0,   0],\n",
              "         [  0,   0,   0,   0,   0,   0,   0,   0,   0, 124, 205,   0,   9, 151,\n",
              "          142,  19,   0,   0,   8, 217,  41,   0,   0,   0,   0,   0,   0,   0],\n",
              "         [  0,   0,   0,   0,   0,   0,   0,   0,   0, 111, 213,   6, 120, 141,\n",
              "            2,   0,   0,   0,   6, 214,  41,   0,   0,   0,   0,   0,   0,   0],\n",
              "         [  0,   0,   0,   0,   0,   0,   0,   0,   0,  42, 254,  75, 117,   3,\n",
              "            0,   0,   0,   0,  50, 204,   7,   0,   0,   0,   0,   0,   0,   0],\n",
              "         [  0,   0,   0,   0,   0,   0,   0,   0,   0,   6, 174, 199,   6,   0,\n",
              "            0,   0,   0,   0, 134, 131,   0,   0,   0,   0,   0,   0,   0,   0],\n",
              "         [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  45, 246, 146,   0,\n",
              "            0,   0,   0,  71, 241,  71,   0,   0,   0,   0,   0,   0,   0,   0],\n",
              "         [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  78, 243, 188,\n",
              "           77,  78, 162, 238,  78,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
              "         [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  29, 140,\n",
              "          225, 199, 167,  28,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
              "         [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "            0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
              "         [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "            0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
              "         [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "            0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
              "         [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "            0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
              "         [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "            0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0]],\n",
              "        dtype=torch.uint8),\n",
              " tensor(6, dtype=torch.uint8))"
            ]
          },
          "metadata": {},
          "execution_count": 65
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from sklearn.model_selection import ParameterGrid\n",
        "\n",
        "# Define your neural network model\n",
        "class NeuralNetwork(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size1, hidden_size2, output_size):\n",
        "        super(NeuralNetwork, self).__init__()\n",
        "        self.fc1 = nn.Linear(input_size, hidden_size1)\n",
        "        self.fc2 = nn.Linear(hidden_size1, hidden_size2)\n",
        "        self.fc3 = nn.Linear(hidden_size2, output_size)\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.relu(self.fc1(x))\n",
        "        x = self.relu(self.fc2(x))\n",
        "        x = self.fc3(x)\n",
        "        return x\n",
        "\n",
        "# Define grid of hyperparameters to tune\n",
        "param_grid = {\n",
        "    'hidden_size1': [128, 256, 512],\n",
        "    'hidden_size2': [64, 128, 256],\n",
        "    'learning_rate': [0.001, 0.01, 0.1]\n",
        "}\n",
        "\n",
        "# Define other training parameters\n",
        "input_size = 28 * 28  # Size of input features\n",
        "output_size = 10      # Number of classes\n",
        "batch_size = 32\n",
        "epochs = 10\n",
        "\n",
        "# Prepare data loaders\n",
        "# Make sure to run the previous code to define train_loader, validation_loader, and test_loader\n",
        "\n",
        "best_accuracy = 0\n",
        "best_params = None\n",
        "\n",
        "# Iterate over parameter grid\n",
        "for params in ParameterGrid(param_grid):\n",
        "    # Initialize model\n",
        "    model = NeuralNetwork(input_size, params['hidden_size1'], params['hidden_size2'], output_size)\n",
        "\n",
        "    # Define loss function and optimizer\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = optim.Adam(model.parameters(), lr=params['learning_rate'])\n",
        "\n",
        "\n",
        "    # Train the model\n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        for inputs, labels in train_loader:\n",
        "            optimizer.zero_grad()\n",
        "            inputs = inputs.float()  # Convert input data to float32\n",
        "            outputs = model(inputs.view(inputs.size(0), -1))\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "        # Validate the model\n",
        "        model.eval()\n",
        "        correct = 0\n",
        "        total = 0\n",
        "        with torch.no_grad():\n",
        "            for inputs, labels in validation_loader:\n",
        "                inputs = inputs.float()  # Convert input data to float32\n",
        "                outputs = model(inputs.view(inputs.size(0), -1))\n",
        "                _, predicted = torch.max(outputs.data, 1)\n",
        "                total += labels.size(0)\n",
        "                correct += (predicted == labels).sum().item()\n",
        "\n",
        "        accuracy = correct / total\n",
        "        print(f'Epoch {epoch+1}/{epochs}, Validation Accuracy: {accuracy}')\n",
        "\n",
        "    # Check if this model is the best so far\n",
        "    if accuracy > best_accuracy:\n",
        "        best_accuracy = accuracy\n",
        "        best_params = params\n",
        "\n",
        "print(f'Best validation accuracy: {best_accuracy}')\n",
        "print(f'Best hyperparameters: {best_params}')\n"
      ],
      "metadata": {
        "id": "BB-0P786VlF9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1f916287-605e-4b7c-8dee-793487bdf1a8"
      },
      "execution_count": 119,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10, Validation Accuracy: 0.9434\n",
            "Epoch 2/10, Validation Accuracy: 0.9614\n",
            "Epoch 3/10, Validation Accuracy: 0.9626\n",
            "Epoch 4/10, Validation Accuracy: 0.964\n",
            "Epoch 5/10, Validation Accuracy: 0.9656\n",
            "Epoch 6/10, Validation Accuracy: 0.9644\n",
            "Epoch 7/10, Validation Accuracy: 0.9704\n",
            "Epoch 8/10, Validation Accuracy: 0.9708\n",
            "Epoch 9/10, Validation Accuracy: 0.9754\n",
            "Epoch 10/10, Validation Accuracy: 0.9632\n",
            "Epoch 1/10, Validation Accuracy: 0.5836\n",
            "Epoch 2/10, Validation Accuracy: 0.9032\n",
            "Epoch 3/10, Validation Accuracy: 0.927\n",
            "Epoch 4/10, Validation Accuracy: 0.9254\n",
            "Epoch 5/10, Validation Accuracy: 0.9336\n",
            "Epoch 6/10, Validation Accuracy: 0.9358\n",
            "Epoch 7/10, Validation Accuracy: 0.936\n",
            "Epoch 8/10, Validation Accuracy: 0.924\n",
            "Epoch 9/10, Validation Accuracy: 0.94\n",
            "Epoch 10/10, Validation Accuracy: 0.9272\n",
            "Epoch 1/10, Validation Accuracy: 0.0986\n",
            "Epoch 2/10, Validation Accuracy: 0.1014\n",
            "Epoch 3/10, Validation Accuracy: 0.1158\n",
            "Epoch 4/10, Validation Accuracy: 0.1158\n",
            "Epoch 5/10, Validation Accuracy: 0.1158\n",
            "Epoch 6/10, Validation Accuracy: 0.1158\n",
            "Epoch 7/10, Validation Accuracy: 0.0972\n",
            "Epoch 8/10, Validation Accuracy: 0.0984\n",
            "Epoch 9/10, Validation Accuracy: 0.0982\n",
            "Epoch 10/10, Validation Accuracy: 0.1158\n",
            "Epoch 1/10, Validation Accuracy: 0.95\n",
            "Epoch 2/10, Validation Accuracy: 0.9578\n",
            "Epoch 3/10, Validation Accuracy: 0.9524\n",
            "Epoch 4/10, Validation Accuracy: 0.9606\n",
            "Epoch 5/10, Validation Accuracy: 0.9664\n",
            "Epoch 6/10, Validation Accuracy: 0.9666\n",
            "Epoch 7/10, Validation Accuracy: 0.9686\n",
            "Epoch 8/10, Validation Accuracy: 0.9668\n",
            "Epoch 9/10, Validation Accuracy: 0.967\n",
            "Epoch 10/10, Validation Accuracy: 0.9694\n",
            "Epoch 1/10, Validation Accuracy: 0.907\n",
            "Epoch 2/10, Validation Accuracy: 0.9056\n",
            "Epoch 3/10, Validation Accuracy: 0.9126\n",
            "Epoch 4/10, Validation Accuracy: 0.917\n",
            "Epoch 5/10, Validation Accuracy: 0.9114\n",
            "Epoch 6/10, Validation Accuracy: 0.9018\n",
            "Epoch 7/10, Validation Accuracy: 0.9056\n",
            "Epoch 8/10, Validation Accuracy: 0.9182\n",
            "Epoch 9/10, Validation Accuracy: 0.8732\n",
            "Epoch 10/10, Validation Accuracy: 0.8962\n",
            "Epoch 1/10, Validation Accuracy: 0.1148\n",
            "Epoch 2/10, Validation Accuracy: 0.1016\n",
            "Epoch 3/10, Validation Accuracy: 0.1016\n",
            "Epoch 4/10, Validation Accuracy: 0.0972\n",
            "Epoch 5/10, Validation Accuracy: 0.0974\n",
            "Epoch 6/10, Validation Accuracy: 0.1148\n",
            "Epoch 7/10, Validation Accuracy: 0.1016\n",
            "Epoch 8/10, Validation Accuracy: 0.1016\n",
            "Epoch 9/10, Validation Accuracy: 0.1148\n",
            "Epoch 10/10, Validation Accuracy: 0.0974\n",
            "Epoch 1/10, Validation Accuracy: 0.9496\n",
            "Epoch 2/10, Validation Accuracy: 0.9506\n",
            "Epoch 3/10, Validation Accuracy: 0.9568\n",
            "Epoch 4/10, Validation Accuracy: 0.9634\n",
            "Epoch 5/10, Validation Accuracy: 0.958\n",
            "Epoch 6/10, Validation Accuracy: 0.9634\n",
            "Epoch 7/10, Validation Accuracy: 0.96\n",
            "Epoch 8/10, Validation Accuracy: 0.9636\n",
            "Epoch 9/10, Validation Accuracy: 0.9608\n",
            "Epoch 10/10, Validation Accuracy: 0.9688\n",
            "Epoch 1/10, Validation Accuracy: 0.3652\n",
            "Epoch 2/10, Validation Accuracy: 0.3038\n",
            "Epoch 3/10, Validation Accuracy: 0.299\n",
            "Epoch 4/10, Validation Accuracy: 0.3062\n",
            "Epoch 5/10, Validation Accuracy: 0.2852\n",
            "Epoch 6/10, Validation Accuracy: 0.2936\n",
            "Epoch 7/10, Validation Accuracy: 0.3038\n",
            "Epoch 8/10, Validation Accuracy: 0.3068\n",
            "Epoch 9/10, Validation Accuracy: 0.2992\n",
            "Epoch 10/10, Validation Accuracy: 0.299\n",
            "Epoch 1/10, Validation Accuracy: 0.0962\n",
            "Epoch 2/10, Validation Accuracy: 0.0974\n",
            "Epoch 3/10, Validation Accuracy: 0.0988\n",
            "Epoch 4/10, Validation Accuracy: 0.0962\n",
            "Epoch 5/10, Validation Accuracy: 0.0974\n",
            "Epoch 6/10, Validation Accuracy: 0.1148\n",
            "Epoch 7/10, Validation Accuracy: 0.1148\n",
            "Epoch 8/10, Validation Accuracy: 0.0962\n",
            "Epoch 9/10, Validation Accuracy: 0.0972\n",
            "Epoch 10/10, Validation Accuracy: 0.0962\n",
            "Epoch 1/10, Validation Accuracy: 0.9536\n",
            "Epoch 2/10, Validation Accuracy: 0.9656\n",
            "Epoch 3/10, Validation Accuracy: 0.9668\n",
            "Epoch 4/10, Validation Accuracy: 0.966\n",
            "Epoch 5/10, Validation Accuracy: 0.9666\n",
            "Epoch 6/10, Validation Accuracy: 0.9712\n",
            "Epoch 7/10, Validation Accuracy: 0.974\n",
            "Epoch 8/10, Validation Accuracy: 0.9724\n",
            "Epoch 9/10, Validation Accuracy: 0.9616\n",
            "Epoch 10/10, Validation Accuracy: 0.9732\n",
            "Epoch 1/10, Validation Accuracy: 0.7014\n",
            "Epoch 2/10, Validation Accuracy: 0.9092\n",
            "Epoch 3/10, Validation Accuracy: 0.9212\n",
            "Epoch 4/10, Validation Accuracy: 0.925\n",
            "Epoch 5/10, Validation Accuracy: 0.9296\n",
            "Epoch 6/10, Validation Accuracy: 0.9192\n",
            "Epoch 7/10, Validation Accuracy: 0.928\n",
            "Epoch 8/10, Validation Accuracy: 0.9286\n",
            "Epoch 9/10, Validation Accuracy: 0.9164\n",
            "Epoch 10/10, Validation Accuracy: 0.9228\n",
            "Epoch 1/10, Validation Accuracy: 0.1148\n",
            "Epoch 2/10, Validation Accuracy: 0.0972\n",
            "Epoch 3/10, Validation Accuracy: 0.0972\n",
            "Epoch 4/10, Validation Accuracy: 0.0974\n",
            "Epoch 5/10, Validation Accuracy: 0.1148\n",
            "Epoch 6/10, Validation Accuracy: 0.1148\n",
            "Epoch 7/10, Validation Accuracy: 0.0988\n",
            "Epoch 8/10, Validation Accuracy: 0.1148\n",
            "Epoch 9/10, Validation Accuracy: 0.1148\n",
            "Epoch 10/10, Validation Accuracy: 0.1148\n",
            "Epoch 1/10, Validation Accuracy: 0.9514\n",
            "Epoch 2/10, Validation Accuracy: 0.9528\n",
            "Epoch 3/10, Validation Accuracy: 0.961\n",
            "Epoch 4/10, Validation Accuracy: 0.9634\n",
            "Epoch 5/10, Validation Accuracy: 0.9606\n",
            "Epoch 6/10, Validation Accuracy: 0.9646\n",
            "Epoch 7/10, Validation Accuracy: 0.9666\n",
            "Epoch 8/10, Validation Accuracy: 0.9668\n",
            "Epoch 9/10, Validation Accuracy: 0.9692\n",
            "Epoch 10/10, Validation Accuracy: 0.9628\n",
            "Epoch 1/10, Validation Accuracy: 0.73\n",
            "Epoch 2/10, Validation Accuracy: 0.8028\n",
            "Epoch 3/10, Validation Accuracy: 0.8184\n",
            "Epoch 4/10, Validation Accuracy: 0.8098\n",
            "Epoch 5/10, Validation Accuracy: 0.84\n",
            "Epoch 6/10, Validation Accuracy: 0.8322\n",
            "Epoch 7/10, Validation Accuracy: 0.8404\n",
            "Epoch 8/10, Validation Accuracy: 0.8388\n",
            "Epoch 9/10, Validation Accuracy: 0.8454\n",
            "Epoch 10/10, Validation Accuracy: 0.8472\n",
            "Epoch 1/10, Validation Accuracy: 0.1148\n",
            "Epoch 2/10, Validation Accuracy: 0.1148\n",
            "Epoch 3/10, Validation Accuracy: 0.0986\n",
            "Epoch 4/10, Validation Accuracy: 0.0962\n",
            "Epoch 5/10, Validation Accuracy: 0.1148\n",
            "Epoch 6/10, Validation Accuracy: 0.0974\n",
            "Epoch 7/10, Validation Accuracy: 0.0972\n",
            "Epoch 8/10, Validation Accuracy: 0.1148\n",
            "Epoch 9/10, Validation Accuracy: 0.0962\n",
            "Epoch 10/10, Validation Accuracy: 0.0974\n",
            "Epoch 1/10, Validation Accuracy: 0.9566\n",
            "Epoch 2/10, Validation Accuracy: 0.9588\n",
            "Epoch 3/10, Validation Accuracy: 0.957\n",
            "Epoch 4/10, Validation Accuracy: 0.954\n",
            "Epoch 5/10, Validation Accuracy: 0.9642\n",
            "Epoch 6/10, Validation Accuracy: 0.963\n",
            "Epoch 7/10, Validation Accuracy: 0.9596\n",
            "Epoch 8/10, Validation Accuracy: 0.968\n",
            "Epoch 9/10, Validation Accuracy: 0.969\n",
            "Epoch 10/10, Validation Accuracy: 0.9694\n",
            "Epoch 1/10, Validation Accuracy: 0.4412\n",
            "Epoch 2/10, Validation Accuracy: 0.601\n",
            "Epoch 3/10, Validation Accuracy: 0.6502\n",
            "Epoch 4/10, Validation Accuracy: 0.646\n",
            "Epoch 5/10, Validation Accuracy: 0.6576\n",
            "Epoch 6/10, Validation Accuracy: 0.6642\n",
            "Epoch 7/10, Validation Accuracy: 0.6984\n",
            "Epoch 8/10, Validation Accuracy: 0.7046\n",
            "Epoch 9/10, Validation Accuracy: 0.6944\n",
            "Epoch 10/10, Validation Accuracy: 0.6428\n",
            "Epoch 1/10, Validation Accuracy: 0.1054\n",
            "Epoch 2/10, Validation Accuracy: 0.1146\n",
            "Epoch 3/10, Validation Accuracy: 0.0988\n",
            "Epoch 4/10, Validation Accuracy: 0.1146\n",
            "Epoch 5/10, Validation Accuracy: 0.1146\n",
            "Epoch 6/10, Validation Accuracy: 0.1146\n",
            "Epoch 7/10, Validation Accuracy: 0.1146\n",
            "Epoch 8/10, Validation Accuracy: 0.0962\n",
            "Epoch 9/10, Validation Accuracy: 0.1146\n",
            "Epoch 10/10, Validation Accuracy: 0.1146\n",
            "Epoch 1/10, Validation Accuracy: 0.9522\n",
            "Epoch 2/10, Validation Accuracy: 0.96\n",
            "Epoch 3/10, Validation Accuracy: 0.9722\n",
            "Epoch 4/10, Validation Accuracy: 0.9676\n",
            "Epoch 5/10, Validation Accuracy: 0.9714\n",
            "Epoch 6/10, Validation Accuracy: 0.9668\n",
            "Epoch 7/10, Validation Accuracy: 0.9714\n",
            "Epoch 8/10, Validation Accuracy: 0.9684\n",
            "Epoch 9/10, Validation Accuracy: 0.9668\n",
            "Epoch 10/10, Validation Accuracy: 0.9684\n",
            "Epoch 1/10, Validation Accuracy: 0.8842\n",
            "Epoch 2/10, Validation Accuracy: 0.9196\n",
            "Epoch 3/10, Validation Accuracy: 0.921\n",
            "Epoch 4/10, Validation Accuracy: 0.928\n",
            "Epoch 5/10, Validation Accuracy: 0.928\n",
            "Epoch 6/10, Validation Accuracy: 0.927\n",
            "Epoch 7/10, Validation Accuracy: 0.9256\n",
            "Epoch 8/10, Validation Accuracy: 0.9306\n",
            "Epoch 9/10, Validation Accuracy: 0.9102\n",
            "Epoch 10/10, Validation Accuracy: 0.9246\n",
            "Epoch 1/10, Validation Accuracy: 0.1406\n",
            "Epoch 2/10, Validation Accuracy: 0.0974\n",
            "Epoch 3/10, Validation Accuracy: 0.1016\n",
            "Epoch 4/10, Validation Accuracy: 0.1016\n",
            "Epoch 5/10, Validation Accuracy: 0.0974\n",
            "Epoch 6/10, Validation Accuracy: 0.1148\n",
            "Epoch 7/10, Validation Accuracy: 0.0962\n",
            "Epoch 8/10, Validation Accuracy: 0.1016\n",
            "Epoch 9/10, Validation Accuracy: 0.1148\n",
            "Epoch 10/10, Validation Accuracy: 0.0974\n",
            "Epoch 1/10, Validation Accuracy: 0.9564\n",
            "Epoch 2/10, Validation Accuracy: 0.968\n",
            "Epoch 3/10, Validation Accuracy: 0.9596\n",
            "Epoch 4/10, Validation Accuracy: 0.9712\n",
            "Epoch 5/10, Validation Accuracy: 0.9706\n",
            "Epoch 6/10, Validation Accuracy: 0.9698\n",
            "Epoch 7/10, Validation Accuracy: 0.9658\n",
            "Epoch 8/10, Validation Accuracy: 0.9752\n",
            "Epoch 9/10, Validation Accuracy: 0.9666\n",
            "Epoch 10/10, Validation Accuracy: 0.9728\n",
            "Epoch 1/10, Validation Accuracy: 0.8722\n",
            "Epoch 2/10, Validation Accuracy: 0.8984\n",
            "Epoch 3/10, Validation Accuracy: 0.91\n",
            "Epoch 4/10, Validation Accuracy: 0.9094\n",
            "Epoch 5/10, Validation Accuracy: 0.9028\n",
            "Epoch 6/10, Validation Accuracy: 0.9094\n",
            "Epoch 7/10, Validation Accuracy: 0.8372\n",
            "Epoch 8/10, Validation Accuracy: 0.9116\n",
            "Epoch 9/10, Validation Accuracy: 0.8248\n",
            "Epoch 10/10, Validation Accuracy: 0.8432\n",
            "Epoch 1/10, Validation Accuracy: 0.1148\n",
            "Epoch 2/10, Validation Accuracy: 0.0972\n",
            "Epoch 3/10, Validation Accuracy: 0.0994\n",
            "Epoch 4/10, Validation Accuracy: 0.0962\n",
            "Epoch 5/10, Validation Accuracy: 0.1148\n",
            "Epoch 6/10, Validation Accuracy: 0.1148\n",
            "Epoch 7/10, Validation Accuracy: 0.1148\n",
            "Epoch 8/10, Validation Accuracy: 0.1148\n",
            "Epoch 9/10, Validation Accuracy: 0.0972\n",
            "Epoch 10/10, Validation Accuracy: 0.1148\n",
            "Epoch 1/10, Validation Accuracy: 0.951\n",
            "Epoch 2/10, Validation Accuracy: 0.956\n",
            "Epoch 3/10, Validation Accuracy: 0.9658\n",
            "Epoch 4/10, Validation Accuracy: 0.9658\n",
            "Epoch 5/10, Validation Accuracy: 0.9754\n",
            "Epoch 6/10, Validation Accuracy: 0.97\n",
            "Epoch 7/10, Validation Accuracy: 0.9678\n",
            "Epoch 8/10, Validation Accuracy: 0.9684\n",
            "Epoch 9/10, Validation Accuracy: 0.9678\n",
            "Epoch 10/10, Validation Accuracy: 0.9728\n",
            "Epoch 1/10, Validation Accuracy: 0.8238\n",
            "Epoch 2/10, Validation Accuracy: 0.8318\n",
            "Epoch 3/10, Validation Accuracy: 0.8178\n",
            "Epoch 4/10, Validation Accuracy: 0.8374\n",
            "Epoch 5/10, Validation Accuracy: 0.8926\n",
            "Epoch 6/10, Validation Accuracy: 0.8914\n",
            "Epoch 7/10, Validation Accuracy: 0.892\n",
            "Epoch 8/10, Validation Accuracy: 0.8952\n",
            "Epoch 9/10, Validation Accuracy: 0.8958\n",
            "Epoch 10/10, Validation Accuracy: 0.8896\n",
            "Epoch 1/10, Validation Accuracy: 0.1656\n",
            "Epoch 2/10, Validation Accuracy: 0.1312\n",
            "Epoch 3/10, Validation Accuracy: 0.1708\n",
            "Epoch 4/10, Validation Accuracy: 0.151\n",
            "Epoch 5/10, Validation Accuracy: 0.203\n",
            "Epoch 6/10, Validation Accuracy: 0.1874\n",
            "Epoch 7/10, Validation Accuracy: 0.1838\n",
            "Epoch 8/10, Validation Accuracy: 0.1302\n",
            "Epoch 9/10, Validation Accuracy: 0.197\n",
            "Epoch 10/10, Validation Accuracy: 0.1448\n",
            "Best validation accuracy: 0.9732\n",
            "Best hyperparameters: {'hidden_size1': 256, 'hidden_size2': 64, 'learning_rate': 0.001}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# import torch\n",
        "# from torchvision import datasets, transforms\n",
        "# from torch.utils.data import DataLoader, random_split\n",
        "\n",
        "# # Define transformations for data augmentation for training data only\n",
        "# transform_train = transforms.Compose([\n",
        "#     transforms.RandomRotation(15),             # Randomly rotate the image by up to 15 degrees\n",
        "#     transforms.RandomHorizontalFlip(),         # Randomly flip the image horizontally\n",
        "#     transforms.RandomCrop(size=28, padding=4), # Randomly crop the image with padding of 4 pixels\n",
        "#     transforms.ToTensor(),                     # Convert the image to tensor\n",
        "# ])\n",
        "\n",
        "# # Define transformation for validation and test set (no augmentation)\n",
        "# transform_test = transforms.Compose([\n",
        "#     transforms.ToTensor(),\n",
        "# ])\n",
        "\n",
        "# # Load the MNIST dataset\n",
        "# mnist_train = datasets.MNIST(root='./data', train=True, download=True, transform=transform_train)\n",
        "# mnist_test = datasets.MNIST(root='./data', train=False, download=True, transform=transform_test)\n",
        "\n",
        "# # Split the training data into training and validation sets\n",
        "# train_size = int(0.8 * len(mnist_train))\n",
        "# valid_size = len(mnist_train) - train_size\n",
        "# train_dataset, valid_dataset = random_split(mnist_train, [train_size, valid_size])\n",
        "\n",
        "# # Define batch sizes for data loaders\n",
        "# batch_size_train = 64\n",
        "# batch_size_valid = 64\n",
        "# batch_size_test = 64\n",
        "\n",
        "# # Create data loaders\n",
        "# train_loader = DataLoader(train_dataset, batch_size=batch_size_train, shuffle=True)\n",
        "# valid_loader = DataLoader(valid_dataset, batch_size=batch_size_valid, shuffle=False)\n",
        "# test_loader = DataLoader(mnist_test, batch_size=batch_size_test, shuffle=False)\n"
      ],
      "metadata": {
        "id": "JRgHUd9KEPrS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "best_params"
      ],
      "metadata": {
        "id": "E2YD1sc4oU4O"
      },
      "execution_count": 118,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Define your neural network model\n",
        "class NeuralNetwork(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size1, hidden_size2, output_size):\n",
        "        super(NeuralNetwork, self).__init__()\n",
        "        self.fc1 = nn.Linear(input_size, hidden_size1)\n",
        "        self.fc2 = nn.Linear(hidden_size1, hidden_size2)\n",
        "        self.fc3 = nn.Linear(hidden_size2, output_size)\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.relu(self.fc1(x))\n",
        "        x = self.relu(self.fc2(x))\n",
        "        x = self.fc3(x)\n",
        "        return x\n",
        "\n",
        "# Define hyperparameters\n",
        "input_size = 28 * 28  # Size of input features\n",
        "output_size = 10      # Number of classes\n",
        "hidden_size1 = best_params['hidden_size1']    # Size of the first hidden layer\n",
        "hidden_size2 = best_params['hidden_size2']    # Size of the second hidden layer\n",
        "learning_rate = best_params['learning_rate']  # Learning rate\n",
        "batch_size = 32       # Batch size\n",
        "epochs = 10           # Number of epochs\n",
        "\n",
        "# Initialize the model\n",
        "model = NeuralNetwork(input_size, hidden_size1, hidden_size2, output_size)\n",
        "\n",
        "# Define loss function and optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(model.parameters(), lr=learning_rate)\n",
        "\n",
        "# Lists to store training and validation loss\n",
        "train_losses = []\n",
        "valid_losses = []\n",
        "\n",
        "# Train the model\n",
        "for epoch in range(epochs):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    for inputs, labels in train_loader:\n",
        "        optimizer.zero_grad()\n",
        "        inputs = inputs.float()\n",
        "        inputs = inputs.view(inputs.size(0), -1)  # Flatten the input images\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        running_loss += loss.item() * inputs.size(0)\n",
        "    train_loss = running_loss / len(train_loader.dataset)\n",
        "    train_losses.append(train_loss)\n",
        "\n",
        "    # Validate the model\n",
        "    model.eval()\n",
        "    valid_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in valid_loader:\n",
        "            inputs = inputs.float()\n",
        "            inputs = inputs.view(inputs.size(0), -1)  # Flatten the input images\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            valid_loss += loss.item() * inputs.size(0)\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "    valid_loss /= len(valid_loader.dataset)\n",
        "    valid_losses.append(valid_loss)\n",
        "\n",
        "    # Print progress\n",
        "    print(f'Epoch {epoch+1}/{epochs}, Training Loss: {train_loss}, Validation Loss: {valid_loss}')\n",
        "\n",
        "# Evaluate the model on the test set\n",
        "model.eval()\n",
        "test_loss = 0.0\n",
        "correct = 0\n",
        "total = 0\n",
        "with torch.no_grad():\n",
        "    for inputs, labels in test_loader:\n",
        "        inputs = inputs.float()\n",
        "        inputs = inputs.view(inputs.size(0), -1)  # Flatten the input images\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        test_loss += loss.item() * inputs.size(0)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "test_loss /= len(test_loader.dataset)\n",
        "test_accuracy = correct / total\n",
        "\n",
        "print(f'Test Loss: {test_loss}, Test Accuracy: {test_accuracy}')\n",
        "\n",
        "# Plot training and validation loss\n",
        "plt.plot(train_losses, label='Training Loss')\n",
        "plt.plot(valid_losses, label='Validation Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.title('Training and Validation Loss')\n",
        "plt.legend()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 673
        },
        "id": "WozrZYpHpX5l",
        "outputId": "d8e247ee-040d-4752-f979-3eb873a0d369"
      },
      "execution_count": 120,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10, Training Loss: 0.6227228971790771, Validation Loss: 0.14537998169660568\n",
            "Epoch 2/10, Training Loss: 0.25342023710577916, Validation Loss: 0.11486055736541748\n",
            "Epoch 3/10, Training Loss: 0.19238864486130733, Validation Loss: 0.09786209877002984\n",
            "Epoch 4/10, Training Loss: 0.15856542258126133, Validation Loss: 0.08660773202627897\n",
            "Epoch 5/10, Training Loss: 0.1361188758124512, Validation Loss: 0.0870899155177176\n",
            "Epoch 6/10, Training Loss: 0.11956024805732772, Validation Loss: 0.07938651231564581\n",
            "Epoch 7/10, Training Loss: 0.1061837030902669, Validation Loss: 0.0747847914300859\n",
            "Epoch 8/10, Training Loss: 0.09561905857030675, Validation Loss: 0.07477521705627442\n",
            "Epoch 9/10, Training Loss: 0.0871016761610491, Validation Loss: 0.07039058756828308\n",
            "Epoch 10/10, Training Loss: 0.07891333179328969, Validation Loss: 0.06888890554830432\n",
            "Test Loss: 0.07287756333239377, Test Accuracy: 0.9574\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install flopth torchmetrics"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3rUOS6__rps0",
        "outputId": "b373e30c-6370-43e8-d537-94c8d3b404ea"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: flopth in /usr/local/lib/python3.10/dist-packages (0.1.5)\n",
            "Requirement already satisfied: torchmetrics in /usr/local/lib/python3.10/dist-packages (1.3.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from flopth) (1.25.2)\n",
            "Requirement already satisfied: tabulate in /usr/local/lib/python3.10/dist-packages (from flopth) (0.9.0)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from flopth) (2.2.1+cu121)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (from flopth) (0.17.1+cu121)\n",
            "Requirement already satisfied: packaging>17.1 in /usr/local/lib/python3.10/dist-packages (from torchmetrics) (24.0)\n",
            "Requirement already satisfied: lightning-utilities>=0.8.0 in /usr/local/lib/python3.10/dist-packages (from torchmetrics) (0.11.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from lightning-utilities>=0.8.0->torchmetrics) (67.7.2)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from lightning-utilities>=0.8.0->torchmetrics) (4.10.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->flopth) (3.13.3)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->flopth) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->flopth) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->flopth) (3.1.3)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch->flopth) (2023.6.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch->flopth) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch->flopth) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch->flopth) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.10/dist-packages (from torch->flopth) (8.9.2.26)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch->flopth) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch->flopth) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch->flopth) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch->flopth) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch->flopth) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.19.3 in /usr/local/lib/python3.10/dist-packages (from torch->flopth) (2.19.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch->flopth) (12.1.105)\n",
            "Requirement already satisfied: triton==2.2.0 in /usr/local/lib/python3.10/dist-packages (from torch->flopth) (2.2.0)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch->flopth) (12.4.127)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision->flopth) (9.4.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->flopth) (2.1.5)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->flopth) (1.3.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install flopth"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X8uIvbm45f_a",
        "outputId": "0e6213ec-bb51-46a4-cd96-a90bfb01d469"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: flopth in /usr/local/lib/python3.10/dist-packages (0.1.5)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from flopth) (1.25.2)\n",
            "Requirement already satisfied: tabulate in /usr/local/lib/python3.10/dist-packages (from flopth) (0.9.0)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from flopth) (2.2.1+cu121)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (from flopth) (0.17.1+cu121)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->flopth) (3.13.3)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch->flopth) (4.10.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->flopth) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->flopth) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->flopth) (3.1.3)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch->flopth) (2023.6.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch->flopth) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch->flopth) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch->flopth) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.10/dist-packages (from torch->flopth) (8.9.2.26)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch->flopth) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch->flopth) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch->flopth) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch->flopth) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch->flopth) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.19.3 in /usr/local/lib/python3.10/dist-packages (from torch->flopth) (2.19.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch->flopth) (12.1.105)\n",
            "Requirement already satisfied: triton==2.2.0 in /usr/local/lib/python3.10/dist-packages (from torch->flopth) (2.2.0)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch->flopth) (12.4.127)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision->flopth) (9.4.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->flopth) (2.1.5)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->flopth) (1.3.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torchprofile\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wLOn4kUeGOLR",
        "outputId": "bb48705a-1333-45d0-ae2f-a42f4588c23b"
      },
      "execution_count": 88,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torchprofile in /usr/local/lib/python3.10/dist-packages (0.0.4)\n",
            "Requirement already satisfied: numpy>=1.14 in /usr/local/lib/python3.10/dist-packages (from torchprofile) (1.25.2)\n",
            "Requirement already satisfied: torch>=1.4 in /usr/local/lib/python3.10/dist-packages (from torchprofile) (2.2.1+cu121)\n",
            "Requirement already satisfied: torchvision>=0.4 in /usr/local/lib/python3.10/dist-packages (from torchprofile) (0.17.1+cu121)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.4->torchprofile) (3.13.3)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.4->torchprofile) (4.10.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.4->torchprofile) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.4->torchprofile) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.4->torchprofile) (3.1.3)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.4->torchprofile) (2023.6.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.4->torchprofile) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.4->torchprofile) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.4->torchprofile) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.10/dist-packages (from torch>=1.4->torchprofile) (8.9.2.26)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.4->torchprofile) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch>=1.4->torchprofile) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch>=1.4->torchprofile) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch>=1.4->torchprofile) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch>=1.4->torchprofile) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.19.3 in /usr/local/lib/python3.10/dist-packages (from torch>=1.4->torchprofile) (2.19.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.4->torchprofile) (12.1.105)\n",
            "Requirement already satisfied: triton==2.2.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.4->torchprofile) (2.2.0)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.4->torchprofile) (12.4.127)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision>=0.4->torchprofile) (9.4.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.4->torchprofile) (2.1.5)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.4->torchprofile) (1.3.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torchprofile import profile_macs\n",
        "import torch\n",
        "\n",
        "# Assuming you have your model defined and named `model`\n",
        "\n",
        "# Count number of parameters\n",
        "num_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "print(\"Number of parameters:\", num_params)\n",
        "\n",
        "# Profile FLOPs\n",
        "inputs = torch.randn(1, 784)  # Example input tensor with appropriate dimensions\n",
        "flops_torch = profile_macs(model, inputs)\n",
        "print(\"FLOPs:\", flops_torch)\n"
      ],
      "metadata": {
        "id": "jG_bx7k65bf_",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 371
        },
        "outputId": "e93939bb-4a95-402c-f303-06368eaaa5e4"
      },
      "execution_count": 133,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of parameters: 214538\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "Expected 3D (unbatched) or 4D (batched) input to conv2d, but got input of size: [1, 784]",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-133-c47a3bbc8d14>\u001b[0m in \u001b[0;36m<cell line: 12>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;31m# Profile FLOPs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m784\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Example input tensor with appropriate dimensions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0mflops_torch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprofile_macs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"FLOPs:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflops_torch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torchprofile/profile.py\u001b[0m in \u001b[0;36mprofile_macs\u001b[0;34m(model, args, kwargs, reduction)\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m     \u001b[0mgraph\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mnode\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mgraph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnodes\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0moperators\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mhandlers\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torchprofile/utils/trace.py\u001b[0m in \u001b[0;36mtrace\u001b[0;34m(model, args, kwargs)\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mwarnings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcatch_warnings\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrecord\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m         \u001b[0mgraph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_trace_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mFlatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0mvariables\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/jit/_trace.py\u001b[0m in \u001b[0;36m_get_trace_graph\u001b[0;34m(f, args, kwargs, strict, _force_outplace, return_inputs, _return_inputs_states)\u001b[0m\n\u001b[1;32m   1294\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1295\u001b[0m         \u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1296\u001b[0;31m     outs = ONNXTracedModule(\n\u001b[0m\u001b[1;32m   1297\u001b[0m         \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstrict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_force_outplace\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_return_inputs_states\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1298\u001b[0m     )(*args, **kwargs)\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1510\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1511\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1512\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1513\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1518\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1519\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1521\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1522\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/jit/_trace.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    136\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_vars\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    137\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 138\u001b[0;31m         graph, out = torch._C._create_graph_by_tracing(\n\u001b[0m\u001b[1;32m    139\u001b[0m             \u001b[0mwrapper\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m             \u001b[0min_vars\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mmodule_state\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/jit/_trace.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args)\u001b[0m\n\u001b[1;32m    127\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_return_inputs_states\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    128\u001b[0m                 \u001b[0minputs_states\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_unflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0min_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0min_desc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 129\u001b[0;31m             \u001b[0mouts\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minner\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mtrace_inputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    130\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_return_inputs_states\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    131\u001b[0m                 \u001b[0minputs_states\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0minputs_states\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrace_inputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1510\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1511\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1512\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1513\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1518\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1519\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1521\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1522\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_slow_forward\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1499\u001b[0m                 \u001b[0mrecording_scopes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1501\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1502\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1503\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mrecording_scopes\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torchprofile/utils/flatten.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1510\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1511\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1512\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1513\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1518\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1519\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1521\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1522\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_slow_forward\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1499\u001b[0m                 \u001b[0mrecording_scopes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1501\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1502\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1503\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mrecording_scopes\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-126-e3f60a80fc72>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpool1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpool2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/flopth/model_viewer.py\u001b[0m in \u001b[0;36mforward_with_hook\u001b[0;34m(module, *args, **kwargs)\u001b[0m\n\u001b[1;32m     70\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mforward_with_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m             \u001b[0;31m# Calculate FLOPs of current module\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 72\u001b[0;31m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward_funcs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     73\u001b[0m             \u001b[0margs_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m             module.flops = torch.from_numpy(\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/flopth/model_viewer.py\u001b[0m in \u001b[0;36mforward_with_hook\u001b[0;34m(module, *args, **kwargs)\u001b[0m\n\u001b[1;32m     70\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mforward_with_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m             \u001b[0;31m# Calculate FLOPs of current module\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 72\u001b[0;31m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward_funcs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     73\u001b[0m             \u001b[0margs_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m             module.flops = torch.from_numpy(\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1510\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1511\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1512\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1513\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1518\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1519\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1521\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1522\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_slow_forward\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1499\u001b[0m                 \u001b[0mrecording_scopes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1501\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1502\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1503\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mrecording_scopes\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    458\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    459\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 460\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_conv_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    461\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    462\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mConv3d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_ConvNd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36m_conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    454\u001b[0m                             \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstride\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    455\u001b[0m                             _pair(0), self.dilation, self.groups)\n\u001b[0;32m--> 456\u001b[0;31m         return F.conv2d(input, weight, bias, self.stride,\n\u001b[0m\u001b[1;32m    457\u001b[0m                         self.padding, self.dilation, self.groups)\n\u001b[1;32m    458\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Expected 3D (unbatched) or 4D (batched) input to conv2d, but got input of size: [1, 784]"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from flopth import flopth\n",
        "flops, params = flopth(model, in_size=(784,),show_detail=True)\n",
        "print(flops, params)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 335
        },
        "id": "1Qt-ZBtrGdvJ",
        "outputId": "e30a2e9e-7693-4687-aad0-446f89bce874"
      },
      "execution_count": 130,
      "outputs": [
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "Expected 3D (unbatched) or 4D (batched) input to conv2d, but got input of size: [1, 784]",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-130-3ca71cdd9052>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mflopth\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mflopth\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mflops\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mflopth\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0min_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m784\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mshow_detail\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mflops\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/flopth/__init__.py\u001b[0m in \u001b[0;36mflopth\u001b[0;34m(model, in_size, inputs, dtype, param_dict, show_detail, bare_number)\u001b[0m\n\u001b[1;32m    212\u001b[0m             \u001b[0minput_list\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    213\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 214\u001b[0;31m     \u001b[0mmv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mModelViewer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparam_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    215\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    216\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mshow_detail\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/flopth/model_viewer.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, model, input_list, param_dict, dtype)\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m     \u001b[0;31m#        self.show_results()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1510\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1511\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1512\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1513\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1518\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1519\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1521\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1522\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-126-e3f60a80fc72>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpool1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpool2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/flopth/model_viewer.py\u001b[0m in \u001b[0;36mforward_with_hook\u001b[0;34m(module, *args, **kwargs)\u001b[0m\n\u001b[1;32m     70\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mforward_with_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m             \u001b[0;31m# Calculate FLOPs of current module\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 72\u001b[0;31m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward_funcs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     73\u001b[0m             \u001b[0margs_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m             module.flops = torch.from_numpy(\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/flopth/model_viewer.py\u001b[0m in \u001b[0;36mforward_with_hook\u001b[0;34m(module, *args, **kwargs)\u001b[0m\n\u001b[1;32m     70\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mforward_with_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m             \u001b[0;31m# Calculate FLOPs of current module\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 72\u001b[0;31m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward_funcs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     73\u001b[0m             \u001b[0margs_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m             module.flops = torch.from_numpy(\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1510\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1511\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1512\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1513\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1518\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1519\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1521\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1522\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    458\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    459\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 460\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_conv_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    461\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    462\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mConv3d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_ConvNd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36m_conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    454\u001b[0m                             \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstride\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    455\u001b[0m                             _pair(0), self.dilation, self.groups)\n\u001b[0;32m--> 456\u001b[0;31m         return F.conv2d(input, weight, bias, self.stride,\n\u001b[0m\u001b[1;32m    457\u001b[0m                         self.padding, self.dilation, self.groups)\n\u001b[1;32m    458\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Expected 3D (unbatched) or 4D (batched) input to conv2d, but got input of size: [1, 784]"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torchmetrics import ConfusionMatrix, Precision, Recall\n",
        "confusion_matrix = ConfusionMatrix(num_classes=10,task=\"multiclass\")\n",
        "precision = Precision(num_classes=10,task=\"multiclass\")\n",
        "recall = Recall(num_classes=10,task=\"multiclass\")\n",
        "\n",
        "with torch.no_grad():\n",
        "    for images, labels in test_loader:\n",
        "        # Move input data to GPU if available\n",
        "        outputs = model(images.float().view(-1, 784))\n",
        "        pred = torch.argmax(outputs, dim=1)\n",
        "        confusion_matrix.update(pred, labels)\n",
        "        precision.update(pred, labels)\n",
        "        recall.update(pred, labels)\n",
        "\n",
        "\n",
        "# Compute and print metrics\n",
        "print(\"Confusion Matrix:\")\n",
        "print(confusion_matrix.compute().numpy())\n",
        "\n",
        "print(\"Precision:\")\n",
        "print(precision.compute())\n",
        "\n",
        "print(\"Recall:\")\n",
        "print(recall.compute())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Eh0A99MEH0Mh",
        "outputId": "7598ce7f-7be1-4ce5-9968-4fb42b143412"
      },
      "execution_count": 124,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Confusion Matrix:\n",
            "[[483   0   2   0   1   2   3   0   2   1]\n",
            " [  0 551   1   1   0   0   1   1   6   0]\n",
            " [  7   2 495   4   6   0   1   3   5   1]\n",
            " [  0   0   3 499   0  13   1   3   2   2]\n",
            " [  0   0   3   0 482   0   2   1   3  10]\n",
            " [  0   0   0   7   1 399   3   0   3   2]\n",
            " [  6   1   2   0   7   1 475   1   1   0]\n",
            " [  1   3   7   1   3   1   0 468   0  12]\n",
            " [  1   0   2   5   4   5   2   4 454   3]\n",
            " [  1   3   1   7  11   2   0   2   4 481]]\n",
            "Precision:\n",
            "tensor(0.9574)\n",
            "Recall:\n",
            "tensor(0.9574)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from torchmetrics import Accuracy\n",
        "from sklearn.model_selection import ParameterGrid\n"
      ],
      "metadata": {
        "id": "KUTeK4r9uQm7"
      },
      "execution_count": 125,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CNN(nn.Module):\n",
        "    def __init__(self, nconv1=16, nconv2=32):\n",
        "        super(CNN, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(1, nconv1, 3, padding='same')\n",
        "        self.conv2 = nn.Conv2d(nconv1, nconv2, 3, padding='same')\n",
        "        self.nn1 = nn.Linear(nconv2 * 7 * 7, 128)\n",
        "        self.nn2 = nn.Linear(128, 64)\n",
        "        self.nn3 = nn.Linear(64, 10)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.pool1 = nn.MaxPool2d(2, 2)\n",
        "        self.pool2 = nn.MaxPool2d(2, 2)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.pool1(self.relu(self.conv1(x)))\n",
        "        x = self.pool2(self.relu(self.conv2(x)))\n",
        "        x = torch.flatten(x, 1)\n",
        "        x = F.relu(self.nn1(x))\n",
        "        x = F.relu(self.nn2(x))\n",
        "        x = self.nn3(x)\n",
        "        return x\n"
      ],
      "metadata": {
        "id": "ypGuURv0uRjY"
      },
      "execution_count": 126,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "best_accuracy = 0\n",
        "best_params = None\n",
        "\n",
        "cnn_param_grid = {\n",
        "    'nconv1': [16, 32, 64],\n",
        "    'nconv2': [32, 64, 128],\n",
        "    'lr': [0.001, 0.01, 0.1]\n",
        "}\n",
        "\n",
        "for params in ParameterGrid(cnn_param_grid):\n",
        "    # Initialize model with current hyperparameters\n",
        "    model = CNN(params['nconv1'], params['nconv2'])\n",
        "\n",
        "    # Define loss function and optimizer\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = optim.Adam(model.parameters(), lr=params['lr'])\n",
        "\n",
        "    # Define accuracy metric\n",
        "    accuracy = Accuracy()\n",
        "\n",
        "    # Train the model\n",
        "    for epoch in range(10):\n",
        "        model.train()\n",
        "        running_train_loss = 0\n",
        "        for inputs, labels in train_loader:\n",
        "            optimizer.zero_grad()\n",
        "            output = model(inputs.unsqueeze(1).float())\n",
        "            loss = criterion(output, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            running_train_loss += loss.item() * inputs.size(0)\n",
        "\n",
        "        model.eval()\n",
        "        correct = 0\n",
        "        total = 0\n",
        "        with torch.no_grad():\n",
        "            for data, target in validation_loader:\n",
        "                output = model(data.unsqueeze(1).float())\n",
        "                _, predicted = torch.max(output, 1)\n",
        "                total += target.size(0)\n",
        "                correct += (predicted == target).sum().item()\n",
        "\n",
        "        # Calculate accuracy using torchmetrics\n",
        "        val_accuracy = correct / total\n",
        "\n",
        "        # Print current hyperparameters and validation accuracy\n",
        "        print(f'Params: {params}, Epoch[{epoch+1}/10], Val_accuracy: {val_accuracy:.3f}')\n",
        "\n",
        "        # Update best accuracy and best parameters\n",
        "        if val_accuracy > best_accuracy:\n",
        "            best_accuracy = val_accuracy\n",
        "            best_params = params\n",
        "\n",
        "print(\"Best Hyperparameters:\", best_params)\n",
        "print(\"Best Validation Accuracy:\", best_accuracy)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 219
        },
        "id": "B212pHQUuMRJ",
        "outputId": "6d3a63d6-07b6-4d97-ceeb-231a2b28db40"
      },
      "execution_count": 128,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "Accuracy.__new__() missing 1 required positional argument: 'task'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-128-aaba3dba58ad>\u001b[0m in \u001b[0;36m<cell line: 10>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0;31m# Define accuracy metric\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m     \u001b[0maccuracy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAccuracy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0;31m# Train the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: Accuracy.__new__() missing 1 required positional argument: 'task'"
          ]
        }
      ]
    }
  ]
}